{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbaf0185-e4a1-4952-a964-8dafb8a01c2a",
   "metadata": {},
   "source": [
    "# Chapter-8 End to End Execution of YOLOv12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a99b56-f261-459a-9154-e2f7f063e907",
   "metadata": {},
   "source": [
    "#### In this notebook, we will use the concepts which we learned in the earlier chapters and try to play with YOLOv12 model in an end-to-end manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d877c16f-a441-4245-be97-cb918175f23e",
   "metadata": {},
   "source": [
    "### Step-1 Exporting YOLOv12 to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e3f1435-5f3b-44cd-adc6-23ac8368e54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov12'...\n",
      "remote: Enumerating objects: 976, done.\u001b[K\n",
      "remote: Counting objects: 100% (221/221), done.\u001b[K\n",
      "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
      "remote: Total 976 (delta 209), reused 190 (delta 189), pack-reused 755 (from 2)\u001b[K\n",
      "Receiving objects: 100% (976/976), 1.61 MiB | 831.00 KiB/s, done.\n",
      "Resolving deltas: 100% (464/464), done.\n",
      "Updating files: 100% (322/322), done.\n",
      "/mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/Ultimate-ONNX-for-Optimizing-Deep-Learning-Models/Chapter-8/yolov12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "# Clone YOLOv12 repository\n",
    "!git clone https://github.com/sunsmarterjie/yolov12.git\n",
    "%cd yolov12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32713554-ff53-4472-9cc7-7ee4c5b5a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are the minimal requirements as we don\n",
    "req = \"\"\"\n",
    "--extra-index-url https://download.pytorch.org/whl/cpu\n",
    "torch==2.2.2\n",
    "torchvision==0.17.2\n",
    "onnx==1.14.0\n",
    "PyYAML==6.0.1\n",
    "# scipy==1.13.0\n",
    "onnxslim==0.1.31\n",
    "onnxruntime==1.18.0\n",
    "opencv-python==4.9.0.80\n",
    "# psutil==5.9.8\n",
    "# py-cpuinfo==9.0.0\n",
    "huggingface-hub==0.23.2\n",
    "safetensors==0.4.3\n",
    "numpy==1.26.4\n",
    "matplotlib\"\"\"\n",
    "\n",
    "with open(\"requirements_minimul.txt\", \"w\") as f:\n",
    "    f.write(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5da06a8-6d4a-49fc-8019-c27533a5f2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch==2.2.2 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from -r requirements_minimul.txt (line 3)) (2.2.2+cpu)\n",
      "Requirement already satisfied: torchvision==0.17.2 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from -r requirements_minimul.txt (line 4)) (0.17.2+cpu)\n",
      "Requirement already satisfied: onnx==1.14.0 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from -r requirements_minimul.txt (line 5)) (1.14.0)\n",
      "Requirement already satisfied: PyYAML==6.0.1 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from -r requirements_minimul.txt (line 6)) (6.0.1)\n",
      "Requirement already satisfied: onnxslim==0.1.31 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from -r requirements_minimul.txt (line 8)) (0.1.31)\n",
      "Requirement already satisfied: onnxruntime==1.18.0 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from -r requirements_minimul.txt (line 9)) (1.18.0)\n",
      "Requirement already satisfied: opencv-python==4.9.0.80 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from -r requirements_minimul.txt (line 10)) (4.9.0.80)\n",
      "Requirement already satisfied: huggingface-hub==0.23.2 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from -r requirements_minimul.txt (line 13)) (0.23.2)\n",
      "Requirement already satisfied: safetensors==0.4.3 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from -r requirements_minimul.txt (line 14)) (0.4.3)\n",
      "Requirement already satisfied: numpy==1.26.4 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from -r requirements_minimul.txt (line 15)) (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from -r requirements_minimul.txt (line 16)) (3.10.0)\n",
      "Requirement already satisfied: jinja2 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from torch==2.2.2->-r requirements_minimul.txt (line 3)) (3.1.6)\n",
      "Requirement already satisfied: filelock in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from torch==2.2.2->-r requirements_minimul.txt (line 3)) (3.17.0)\n",
      "Requirement already satisfied: fsspec in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from torch==2.2.2->-r requirements_minimul.txt (line 3)) (2024.12.0)\n",
      "Requirement already satisfied: sympy in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from torch==2.2.2->-r requirements_minimul.txt (line 3)) (1.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from torch==2.2.2->-r requirements_minimul.txt (line 3)) (4.12.2)\n",
      "Requirement already satisfied: networkx in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from torch==2.2.2->-r requirements_minimul.txt (line 3)) (3.4.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from torchvision==0.17.2->-r requirements_minimul.txt (line 4)) (10.2.0)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from onnx==1.14.0->-r requirements_minimul.txt (line 5)) (6.30.2)\n",
      "Requirement already satisfied: packaging in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from onnxslim==0.1.31->-r requirements_minimul.txt (line 8)) (24.2)\n",
      "Requirement already satisfied: flatbuffers in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from onnxruntime==1.18.0->-r requirements_minimul.txt (line 9)) (25.2.10)\n",
      "Requirement already satisfied: coloredlogs in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from onnxruntime==1.18.0->-r requirements_minimul.txt (line 9)) (15.0.1)\n",
      "Requirement already satisfied: requests in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from huggingface-hub==0.23.2->-r requirements_minimul.txt (line 13)) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from huggingface-hub==0.23.2->-r requirements_minimul.txt (line 13)) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from matplotlib->-r requirements_minimul.txt (line 16)) (2.9.0.post0)\n",
      "Requirement already satisfied: cycler>=0.10 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from matplotlib->-r requirements_minimul.txt (line 16)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from matplotlib->-r requirements_minimul.txt (line 16)) (4.55.6)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from matplotlib->-r requirements_minimul.txt (line 16)) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from matplotlib->-r requirements_minimul.txt (line 16)) (3.2.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from matplotlib->-r requirements_minimul.txt (line 16)) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->-r requirements_minimul.txt (line 16)) (1.17.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from coloredlogs->onnxruntime==1.18.0->-r requirements_minimul.txt (line 9)) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from jinja2->torch==2.2.2->-r requirements_minimul.txt (line 3)) (3.0.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from requests->huggingface-hub==0.23.2->-r requirements_minimul.txt (line 13)) (2.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from requests->huggingface-hub==0.23.2->-r requirements_minimul.txt (line 13)) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from requests->huggingface-hub==0.23.2->-r requirements_minimul.txt (line 13)) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from requests->huggingface-hub==0.23.2->-r requirements_minimul.txt (line 13)) (3.4.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from sympy->torch==2.2.2->-r requirements_minimul.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: thop in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (0.1.1.post2209072238)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements_minimul.txt\n",
    "!pip install thop --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4385c06f-4ee7-4c99-b136-c8890c034ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention is not available on this device. Using scaled_dot_product_attention instead.\n",
      "Downloading https://github.com/sunsmarterjie/yolov12/releases/download/turbo/yolov12n.pt to 'yolov12n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5.26M/5.26M [00:00<00:00, 31.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ 'source' is missing. Using 'source=/mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/Ultimate-ONNX-for-Optimizing-Deep-Learning-Models/Chapter-8/yolov12/ultralytics/assets'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/2 /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/Ultimate-ONNX-for-Optimizing-Deep-Learning-Models/Chapter-8/yolov12/ultralytics/assets/bus.jpg: 640x480 4 persons, 1 bus, 777.7ms\n",
      "image 2/2 /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/Ultimate-ONNX-for-Optimizing-Deep-Learning-Models/Chapter-8/yolov12/ultralytics/assets/zidane.jpg: 384x640 2 persons, 1 tie, 79.6ms\n",
      "Speed: 23.4ms preprocess, 428.6ms inference, 29.4ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "# Run YOLOv12 model in Pytorch first.\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov12n.pt')\n",
    "res = model.predict()\n",
    "\n",
    "for i, r in enumerate(res):\n",
    "    r.save(f\"res_{i}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbe9897c-049a-4a28-973a-3973405474f6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "def display_side_by_side(img_path1, img_path2, caption1=\"Image 1\", caption2=\"Image 2\", \n",
    "                        width=512, title=\"Image Comparison\"):\n",
    "    \"\"\"\n",
    "    Display two images side by side with centered captions and a title.\n",
    "    \n",
    "    Parameters:\n",
    "    - img_path1: Path to the first image\n",
    "    - img_path2: Path to the second image\n",
    "    - caption1: Caption for the first image\n",
    "    - caption2: Caption for the second image\n",
    "    - width: Display width for each image in pixels\n",
    "    - title: Title for the entire comparison\n",
    "    \"\"\"\n",
    "    html = f\"\"\"\n",
    "    <div style=\"text-align: center; margin: 20px 0;\">\n",
    "        <h3>{title}</h3>\n",
    "        <table style=\"margin: 0 auto; border-collapse: collapse;\">\n",
    "            <tr>\n",
    "                <td style=\"padding: 10px; text-align: center; vertical-align: top;\">\n",
    "                    <div style=\"width: {width}px; margin: 0 auto;\">\n",
    "                        <img src=\"{img_path1}\" alt=\"{caption1}\" style=\"max-width: 100%; height: auto; display: block; margin: 0 auto;\"/>\n",
    "                        <p style=\"text-align: center; margin-top: 8px; font-weight: bold;\">{caption1}</p>\n",
    "                    </div>\n",
    "                </td>\n",
    "                <td style=\"padding: 10px; text-align: center; vertical-align: top;\">\n",
    "                    <div style=\"width: {width}px; margin: 0 auto;\">\n",
    "                        <img src=\"{img_path2}\" alt=\"{caption2}\" style=\"max-width: 100%; height: auto; display: block; margin: 0 auto;\"/>\n",
    "                        <p style=\"text-align: center; margin-top: 8px; font-weight: bold;\">{caption2}</p>\n",
    "                    </div>\n",
    "                </td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41a35645-305c-49c6-9d50-1fa9e7fc9930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"text-align: center; margin: 20px 0;\">\n",
       "        <h3>Pytorch Outputs</h3>\n",
       "        <table style=\"margin: 0 auto; border-collapse: collapse;\">\n",
       "            <tr>\n",
       "                <td style=\"padding: 10px; text-align: center; vertical-align: top;\">\n",
       "                    <div style=\"width: 512px; margin: 0 auto;\">\n",
       "                        <img src=\"./yolov12/ultralytics/assets/zidane.jpg\" alt=\"Original Image\" style=\"max-width: 100%; height: auto; display: block; margin: 0 auto;\"/>\n",
       "                        <p style=\"text-align: center; margin-top: 8px; font-weight: bold;\">Original Image</p>\n",
       "                    </div>\n",
       "                </td>\n",
       "                <td style=\"padding: 10px; text-align: center; vertical-align: top;\">\n",
       "                    <div style=\"width: 512px; margin: 0 auto;\">\n",
       "                        <img src=\"./yolov12/res_1.jpg\" alt=\"Image with detections\" style=\"max-width: 100%; height: auto; display: block; margin: 0 auto;\"/>\n",
       "                        <p style=\"text-align: center; margin-top: 8px; font-weight: bold;\">Image with detections</p>\n",
       "                    </div>\n",
       "                </td>\n",
       "            </tr>\n",
       "        </table>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_side_by_side(\n",
    "    \"./yolov12/ultralytics/assets/zidane.jpg\",\n",
    "    \"./yolov12/res_1.jpg\",\n",
    "    \"Original Image\",\n",
    "    \"Image with detections\",\n",
    "    title=\"Pytorch Outputs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "349c96a6-9d5a-4768-8e7f-041c2472c624",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.63 🚀 Python-3.10.12 torch-2.2.2+cpu CPU (AMD Ryzen 5 5600H with Radeon Graphics)\n",
      "YOLOv12n summary (fused): 376 layers, 2,542,440 parameters, 0 gradients, 6.0 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov12n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (5.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.0 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.31...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 3.4s, saved as 'yolov12n.onnx' (10.0 MB)\n",
      "\n",
      "Export complete (4.1s)\n",
      "Results saved to \u001b[1m/mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/Ultimate-ONNX-for-Optimizing-Deep-Learning-Models/Chapter-8/yolov12\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolov12n.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolov12n.onnx imgsz=640 data=None  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yolov12n.onnx'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export to ONNX\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov12n.pt')\n",
    "model.export(format=\"onnx\", device=\"cpu\", simplify=True, nms=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5d0b955-4226-4719-bbd2-63a3f7e0a311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplifying\u001b[33m...\u001b[0m\n",
      "Finish! Here is the difference:\n",
      "┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOriginal Model\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSimplified Model\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
      "│ Add        │ 36             │ 36               │\n",
      "│ Concat     │ 22             │ 22               │\n",
      "│ Constant   │ 367            │ \u001b[1;32m284             \u001b[0m │\n",
      "│ Conv       │ 128            │ 128              │\n",
      "│ Div        │ 9              │ 9                │\n",
      "│ Exp        │ 8              │ 8                │\n",
      "│ MatMul     │ 16             │ 16               │\n",
      "│ Mul        │ 90             │ 90               │\n",
      "│ ReduceMax  │ 8              │ 8                │\n",
      "│ ReduceSum  │ 8              │ 8                │\n",
      "│ Reshape    │ 57             │ 57               │\n",
      "│ Resize     │ 2              │ 2                │\n",
      "│ Sigmoid    │ 82             │ 82               │\n",
      "│ Slice      │ 18             │ 18               │\n",
      "│ Softmax    │ 1              │ 1                │\n",
      "│ Split      │ 4              │ 4                │\n",
      "│ Sub        │ 10             │ 10               │\n",
      "│ Transpose  │ 65             │ 65               │\n",
      "│ Model Size │ 10.0MiB        │ \u001b[1;32m9.9MiB          \u001b[0m │\n",
      "└────────────┴────────────────┴──────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Optimize the model using Onnx-Simplifier\n",
    "!python -m onnxsim \"./yolov12n.onnx\" \"./yolov12n.onnx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349cc78c-ccb4-4b7d-bde9-3eed2e778f99",
   "metadata": {},
   "source": [
    "### Step-2 Execute FP32 model using ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e2f071b-b359-4d9b-84e0-fdcd95bd5645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/Ultimate-ONNX-for-Optimizing-Deep-Learning-Models/Chapter-8\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bceadefa-bbbd-4991-8b8e-dc483911c2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 0.105s\n",
      "Saved results to output_fp32.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from yolo_helper.inference import YOLOInference\n",
    "from yolo_helper.visualization import draw_detections\n",
    "from yolo_helper.postprocessing import get_class_names\n",
    "\n",
    "# Configuration\n",
    "MODEL_PATH = \"yolov12/yolov12n.onnx\"\n",
    "IMAGE_PATH = \"./yolov12/ultralytics/assets/zidane.jpg\"\n",
    "CLASS_NAMES = get_class_names()  # COCO class names\n",
    "OUTPUT_PATH = \"output_fp32.jpg\"\n",
    "\n",
    "# Initialize detector\n",
    "detector = YOLOInference(MODEL_PATH)\n",
    "\n",
    "# Run inference\n",
    "result = detector(IMAGE_PATH, conf_thresh=0.25, iou_thresh=0.45)\n",
    "\n",
    "if result is not None:\n",
    "    image, detections = result\n",
    "    \n",
    "    # Draw and save results\n",
    "    output_image = draw_detections(image, detections, CLASS_NAMES)\n",
    "    cv2.imwrite(OUTPUT_PATH, output_image)\n",
    "    print(f\"Saved results to {OUTPUT_PATH}\")\n",
    "else:\n",
    "    print(\"No detections found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd3de7c0-857e-4bf3-a7df-8bda08399aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"text-align: center; margin: 20px 0;\">\n",
       "        <h3>ONNX FP32 Outputs</h3>\n",
       "        <table style=\"margin: 0 auto; border-collapse: collapse;\">\n",
       "            <tr>\n",
       "                <td style=\"padding: 10px; text-align: center; vertical-align: top;\">\n",
       "                    <div style=\"width: 512px; margin: 0 auto;\">\n",
       "                        <img src=\"./yolov12/ultralytics/assets/zidane.jpg\" alt=\"Original Image\" style=\"max-width: 100%; height: auto; display: block; margin: 0 auto;\"/>\n",
       "                        <p style=\"text-align: center; margin-top: 8px; font-weight: bold;\">Original Image</p>\n",
       "                    </div>\n",
       "                </td>\n",
       "                <td style=\"padding: 10px; text-align: center; vertical-align: top;\">\n",
       "                    <div style=\"width: 512px; margin: 0 auto;\">\n",
       "                        <img src=\"./output_fp32.jpg\" alt=\"Image with detections\" style=\"max-width: 100%; height: auto; display: block; margin: 0 auto;\"/>\n",
       "                        <p style=\"text-align: center; margin-top: 8px; font-weight: bold;\">Image with detections</p>\n",
       "                    </div>\n",
       "                </td>\n",
       "            </tr>\n",
       "        </table>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_side_by_side(\n",
    "    \"./yolov12/ultralytics/assets/zidane.jpg\",\n",
    "    \"./output_fp32.jpg\",\n",
    "    \"Original Image\",\n",
    "    \"Image with detections\",\n",
    "    title=\"ONNX FP32 Outputs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4e946f-9cc0-4e5c-91e6-08ac7779af66",
   "metadata": {},
   "source": [
    "### Step-3 Apply Static Quantization to model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e852153-bbdd-4a4c-a3b8-c30f4da52c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val2017.zip already exists - skipping download\n",
      "val2017 directory already exists - skipping extraction\n"
     ]
    }
   ],
   "source": [
    "# Download coco-val2017 dataset for calibration.\n",
    "\n",
    "command = \"\"\"\n",
    "# Create datasets directory if it doesn't exist\n",
    "mkdir -p ./datasets\n",
    "\n",
    "# Download coco-val2017 dataset only if zip file doesn't exist\n",
    "if [ ! -f ./datasets/val2017.zip ]; then\n",
    "    wget http://images.cocodataset.org/zips/val2017.zip -O ./datasets/val2017.zip\n",
    "else\n",
    "    echo \"val2017.zip already exists - skipping download\"\n",
    "fi\n",
    "\n",
    "# Extract only if the directory doesn't exist\n",
    "if [ ! -d ./datasets/val2017 ]; then\n",
    "    unzip ./datasets/val2017.zip -d ./datasets/\n",
    "else\n",
    "    echo \"val2017 directory already exists - skipping extraction\"\n",
    "fi\n",
    "\"\"\"\n",
    "\n",
    "with open(\"coco_val_2017_download.sh\", \"w\") as f:\n",
    "    f.writelines(command)\n",
    "    \n",
    "!./coco_val_2017_download.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "880efb57-39a3-4a9f-a00a-03b6479d199b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic Quantized model saved at: ./yolov12/yolov12n_int8_dynamic_quant.onnx\n",
      "Static Quantized model saved at: ./yolov12/yolov12n_int8_static_quant.onnx\n",
      "FP32 Model Size: 9.95 MB\n",
      "Dynamic Quantized Model Size: 2.90 MB\n",
      "Static Quantized Model Size: 3.05 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from yolo_helper.dataloader import COCODataset, YoloDataReader\n",
    "from onnxruntime.quantization import quantize_static, quantize_dynamic, QuantType, CalibrationMethod\n",
    "from onnxruntime.quantization.shape_inference import quant_pre_process\n",
    "\n",
    "dataset = COCODataset(\"./datasets/val2017\", sample=100)\n",
    "data_reader = YoloDataReader(dataset)\n",
    "\n",
    "fp32_path = \"./yolov12/yolov12n.onnx\"\n",
    "fp32_path_preproc = \"./yolov12/yolov12n_preproc.onnx\"\n",
    "int8_path_dynamic_quant = \"./yolov12/yolov12n_int8_dynamic_quant.onnx\"\n",
    "int8_path_static_quant = \"./yolov12/yolov12n_int8_static_quant.onnx\"\n",
    "\n",
    "# Firstly, apply shape inference and onnxruntime model optimization before quantizing the model.\n",
    "quant_pre_process(fp32_path, fp32_path_preproc, skip_symbolic_shape=True)\n",
    "\n",
    "# Apply dynamic quantization\n",
    "quantized_model = quantize_dynamic(\n",
    "    model_input=fp32_path_preproc,            # Input ONNX model\n",
    "    model_output=int8_path_dynamic_quant,     # Output quantized model\n",
    "    weight_type=QuantType.QUInt8              # Quantize only weights to uint8, activations will be quantize during runtime\n",
    ")\n",
    "\n",
    "print(f\"Dynamic Quantized model saved at: {int8_path_dynamic_quant}\")\n",
    "\n",
    "\n",
    "# Apply static quantization\n",
    "quantize_static(\n",
    "    model_input=fp32_path_preproc,          # Input ONNX model\n",
    "    model_output=int8_path_static_quant,    # Output quantized model\n",
    "    calibration_data_reader=data_reader,\n",
    "    weight_type=QuantType.QInt8,            # Quantize weights to int8\n",
    "    activation_type=QuantType.QInt8,        # Quantize activations to int8\n",
    "    calibrate_method=CalibrationMethod.MinMax,\n",
    "    extra_options={\"CalibStridedMinMax\": 4}    # Process 4 images at a time for calibration\n",
    ")\n",
    "\n",
    "print(f\"Static Quantized model saved at: {int8_path_static_quant}\")\n",
    "\n",
    "# Compare model sizes\n",
    "fp32_size = os.path.getsize(fp32_path) / 1024 / 1024\n",
    "dynamic_quant_size = os.path.getsize(int8_path_dynamic_quant) / 1024 / 1024\n",
    "static_quant_size = os.path.getsize(int8_path_static_quant) / 1024 / 1024\n",
    "\n",
    "print(f\"FP32 Model Size: {fp32_size:.2f} MB\")\n",
    "print(f\"Dynamic Quantized Model Size: {dynamic_quant_size:.2f} MB\")\n",
    "print(f\"Static Quantized Model Size: {static_quant_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9199700b-c291-4c8a-861e-963b602b33cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplifying\u001b[33m...\u001b[0m\n",
      "Finish! Here is the difference:\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m                     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOriginal Model\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSimplified Model\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
      "│ Add                   │ 163            │ 163              │\n",
      "│ Cast                  │ 128            │ 128              │\n",
      "│ Concat                │ 22             │ 22               │\n",
      "│ Constant              │ 675            │ \u001b[1;32m475             \u001b[0m │\n",
      "│ ConvInteger           │ 128            │ 128              │\n",
      "│ Div                   │ 9              │ 9                │\n",
      "│ DynamicQuantizeLinear │ 111            │ 111              │\n",
      "│ Exp                   │ 8              │ 8                │\n",
      "│ MatMul                │ 16             │ 16               │\n",
      "│ Mul                   │ 346            │ 346              │\n",
      "│ ReduceMax             │ 8              │ 8                │\n",
      "│ ReduceSum             │ 8              │ 8                │\n",
      "│ Reshape               │ 184            │ \u001b[1;32m57              \u001b[0m │\n",
      "│ Resize                │ 2              │ 2                │\n",
      "│ Sigmoid               │ 82             │ 82               │\n",
      "│ Slice                 │ 18             │ 18               │\n",
      "│ Softmax               │ 1              │ 1                │\n",
      "│ Split                 │ 4              │ 4                │\n",
      "│ Sub                   │ 10             │ 10               │\n",
      "│ Transpose             │ 53             │ 53               │\n",
      "│ Model Size            │ 2.9MiB         │ 2.9MiB           │\n",
      "└───────────────────────┴────────────────┴──────────────────┘\n",
      "Simplifying\u001b[33m...\u001b[0m\n",
      "Finish! Here is the difference:\n",
      "┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m                \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOriginal Model\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSimplified Model\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n",
      "│ Add              │ 36             │ 36               │\n",
      "│ Concat           │ 22             │ 22               │\n",
      "│ Constant         │ 1616           │ \u001b[1;32m1012            \u001b[0m │\n",
      "│ Conv             │ 128            │ 128              │\n",
      "│ DequantizeLinear │ 742            │ 742              │\n",
      "│ Div              │ 9              │ 9                │\n",
      "│ Exp              │ 8              │ 8                │\n",
      "│ MatMul           │ 16             │ 16               │\n",
      "│ Mul              │ 90             │ 90               │\n",
      "│ QuantizeLinear   │ 484            │ 484              │\n",
      "│ ReduceMax        │ 8              │ 8                │\n",
      "│ ReduceSum        │ 8              │ 8                │\n",
      "│ Reshape          │ 57             │ 57               │\n",
      "│ Resize           │ 2              │ 2                │\n",
      "│ Sigmoid          │ 82             │ 82               │\n",
      "│ Slice            │ 18             │ 18               │\n",
      "│ Softmax          │ 1              │ 1                │\n",
      "│ Split            │ 4              │ 4                │\n",
      "│ Sub              │ 10             │ 10               │\n",
      "│ Transpose        │ 53             │ 53               │\n",
      "│ Model Size       │ 3.1MiB         │ 3.1MiB           │\n",
      "└──────────────────┴────────────────┴──────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Simplify quantized models before using\n",
    "\n",
    "!python -m onnxsim ./yolov12/yolov12n_int8_dynamic_quant.onnx ./yolov12/yolov12n_int8_dynamic_quant.onnx\n",
    "!python -m onnxsim ./yolov12/yolov12n_int8_static_quant.onnx ./yolov12/yolov12n_int8_static_quant.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56547780-9822-4662-a916-cbf3ca2d01cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 0.159s\n",
      "No detections found\n"
     ]
    }
   ],
   "source": [
    "# Run Static Quantized Model\n",
    "\n",
    "# Configuration\n",
    "MODEL_PATH = int8_path_static_quant\n",
    "IMAGE_PATH = \"./yolov12/ultralytics/assets/zidane.jpg\"\n",
    "CLASS_NAMES = get_class_names()  # COCO class names\n",
    "OUTPUT_PATH = \"./output_int8_static.jpg\"\n",
    "\n",
    "# Initialize detector\n",
    "detector = YOLOInference(MODEL_PATH)\n",
    "\n",
    "# Run inference with reduced conf_threshold.\n",
    "result = detector(IMAGE_PATH, conf_thresh=0.05, iou_thresh=0.45)\n",
    "\n",
    "if result is not None:\n",
    "    image, detections = result\n",
    "    \n",
    "    # Draw and save results\n",
    "    output_image = draw_detections(image, detections, CLASS_NAMES)\n",
    "    cv2.imwrite(OUTPUT_PATH, output_image)\n",
    "    print(f\"Saved results to {OUTPUT_PATH}\")\n",
    "else:\n",
    "    print(\"No detections found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5efb03f-c1c8-4871-8812-8a3eefe589b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 0.110s\n",
      "Saved results to ./output_int8_dynamic.jpg\n"
     ]
    }
   ],
   "source": [
    "# Run Dynamic Quantized Model\n",
    "\n",
    "# Configuration\n",
    "MODEL_PATH = int8_path_dynamic_quant\n",
    "IMAGE_PATH = \"./yolov12/ultralytics/assets/zidane.jpg\"\n",
    "CLASS_NAMES = get_class_names()  # COCO class names\n",
    "OUTPUT_PATH = \"./output_int8_dynamic.jpg\"\n",
    "\n",
    "# Initialize detector\n",
    "detector = YOLOInference(MODEL_PATH)\n",
    "\n",
    "# Run inference\n",
    "result = detector(IMAGE_PATH, conf_thresh=0.25, iou_thresh=0.45)\n",
    "\n",
    "if result is not None:\n",
    "    image, detections = result\n",
    "    \n",
    "    # Draw and save results\n",
    "    output_image = draw_detections(image, detections, CLASS_NAMES)\n",
    "    cv2.imwrite(OUTPUT_PATH, output_image)\n",
    "    print(f\"Saved results to {OUTPUT_PATH}\")\n",
    "else:\n",
    "    print(\"No detections found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25d1c439-4373-4d2a-93f1-a764d9e3bbde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"text-align: center; margin: 20px 0;\">\n",
       "        <h3>ONNX INT8 Dynamic Quantized Outputs</h3>\n",
       "        <table style=\"margin: 0 auto; border-collapse: collapse;\">\n",
       "            <tr>\n",
       "                <td style=\"padding: 10px; text-align: center; vertical-align: top;\">\n",
       "                    <div style=\"width: 512px; margin: 0 auto;\">\n",
       "                        <img src=\"./yolov12/ultralytics/assets/zidane.jpg\" alt=\"Original Image\" style=\"max-width: 100%; height: auto; display: block; margin: 0 auto;\"/>\n",
       "                        <p style=\"text-align: center; margin-top: 8px; font-weight: bold;\">Original Image</p>\n",
       "                    </div>\n",
       "                </td>\n",
       "                <td style=\"padding: 10px; text-align: center; vertical-align: top;\">\n",
       "                    <div style=\"width: 512px; margin: 0 auto;\">\n",
       "                        <img src=\"./output_int8_dynamic.jpg\" alt=\"Image with detections\" style=\"max-width: 100%; height: auto; display: block; margin: 0 auto;\"/>\n",
       "                        <p style=\"text-align: center; margin-top: 8px; font-weight: bold;\">Image with detections</p>\n",
       "                    </div>\n",
       "                </td>\n",
       "            </tr>\n",
       "        </table>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_side_by_side(\n",
    "    \"./yolov12/ultralytics/assets/zidane.jpg\",\n",
    "    \"./output_int8_dynamic.jpg\",\n",
    "    \"Original Image\",\n",
    "    \"Image with detections\",\n",
    "    title=\"ONNX INT8 Dynamic Quantized Outputs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f52ad88-2654-46b8-8678-f92a8bb4cbe3",
   "metadata": {},
   "source": [
    "### Step-4 Debug Static Quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec1adefd-99bb-4ace-a6c5-1052843a8410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Static Quantized model saved at: ./yolov12/yolov12n_int8_static_quant_w_concat_fp32.onnx\n"
     ]
    }
   ],
   "source": [
    "int8_path_static_quant_w_concat_fp32 = \"./yolov12/yolov12n_int8_static_quant_w_concat_fp32.onnx\"\n",
    "\n",
    "# Rewind the datareader so that it can be used again.\n",
    "data_reader.rewind()\n",
    "\n",
    "# Add last Concat node which concatenates boxes prediction and scores prediction.\n",
    "# Boxes prediction is in [0-640] range, while scores prediction is in [0-1] range. \n",
    "# This makes it difficult to quantize. Hence, keeping this node in FP32 precision\n",
    "# recovers the lost accuracy.\n",
    "quantize_static(\n",
    "    model_input=fp32_path_preproc,          # Input ONNX model\n",
    "    model_output=int8_path_static_quant_w_concat_fp32,    # Output quantized model\n",
    "    calibration_data_reader=data_reader,\n",
    "    weight_type=QuantType.QInt8,            # Quantize weights to int8\n",
    "    activation_type=QuantType.QInt8,        # Quantize activations to int8\n",
    "    calibrate_method=CalibrationMethod.MinMax,\n",
    "    nodes_to_exclude=[\"/model.21/Concat_5\"],\n",
    "    extra_options={\"CalibStridedMinMax\": 4}    # Process 4 images at a time for calibration\n",
    ")\n",
    "\n",
    "print(f\"Updated Static Quantized model saved at: {int8_path_static_quant_w_concat_fp32}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24adb7ae-4718-4d96-bc55-0ebbaa161a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 0.188s\n",
      "Saved results to ./output_int8_static_w_concat_fp32.jpg\n"
     ]
    }
   ],
   "source": [
    "# Run Updated Static Quantized Model\n",
    "\n",
    "# Configuration\n",
    "MODEL_PATH = int8_path_static_quant_w_concat_fp32\n",
    "IMAGE_PATH = \"./yolov12/ultralytics/assets/zidane.jpg\"\n",
    "CLASS_NAMES = get_class_names()  # COCO class names\n",
    "OUTPUT_PATH = \"./output_int8_static_w_concat_fp32.jpg\"\n",
    "\n",
    "# Initialize detector\n",
    "detector = YOLOInference(MODEL_PATH)\n",
    "\n",
    "# Run inference with 0.25 conf_threshold\n",
    "result = detector(IMAGE_PATH, conf_thresh=0.25, iou_thresh=0.45)\n",
    "\n",
    "if result is not None:\n",
    "    image, detections = result\n",
    "    \n",
    "    # Draw and save results\n",
    "    output_image = draw_detections(image, detections, CLASS_NAMES)\n",
    "    cv2.imwrite(OUTPUT_PATH, output_image)\n",
    "    print(f\"Saved results to {OUTPUT_PATH}\")\n",
    "else:\n",
    "    print(\"No detections found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1e2cc6f-6af7-4a8d-ab7c-3fe32606e90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"text-align: center; margin: 20px 0;\">\n",
       "        <h3>ONNX INT8 Static Quantized Outputs</h3>\n",
       "        <table style=\"margin: 0 auto; border-collapse: collapse;\">\n",
       "            <tr>\n",
       "                <td style=\"padding: 10px; text-align: center; vertical-align: top;\">\n",
       "                    <div style=\"width: 512px; margin: 0 auto;\">\n",
       "                        <img src=\"./yolov12/ultralytics/assets/zidane.jpg\" alt=\"Original Image\" style=\"max-width: 100%; height: auto; display: block; margin: 0 auto;\"/>\n",
       "                        <p style=\"text-align: center; margin-top: 8px; font-weight: bold;\">Original Image</p>\n",
       "                    </div>\n",
       "                </td>\n",
       "                <td style=\"padding: 10px; text-align: center; vertical-align: top;\">\n",
       "                    <div style=\"width: 512px; margin: 0 auto;\">\n",
       "                        <img src=\"./output_int8_static_w_concat_fp32.jpg\" alt=\"Image with detections\" style=\"max-width: 100%; height: auto; display: block; margin: 0 auto;\"/>\n",
       "                        <p style=\"text-align: center; margin-top: 8px; font-weight: bold;\">Image with detections</p>\n",
       "                    </div>\n",
       "                </td>\n",
       "            </tr>\n",
       "        </table>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage:\n",
    "display_side_by_side(\n",
    "    \"./yolov12/ultralytics/assets/zidane.jpg\",\n",
    "    \"./output_int8_static_w_concat_fp32.jpg\",\n",
    "    \"Original Image\",\n",
    "    \"Image with detections\",\n",
    "    title=\"ONNX INT8 Static Quantized Outputs\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
