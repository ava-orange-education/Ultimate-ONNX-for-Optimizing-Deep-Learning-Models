{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbaf0185-e4a1-4952-a964-8dafb8a01c2a",
   "metadata": {},
   "source": [
    "# Chapter-8 End to End Execution of YOLOv12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a99b56-f261-459a-9154-e2f7f063e907",
   "metadata": {},
   "source": [
    "#### In this notebook, we will use the concepts which we learned in the earlier chapters and try to play with YOLOv12 model in an end-to-end manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d877c16f-a441-4245-be97-cb918175f23e",
   "metadata": {},
   "source": [
    "### Step-1 Exporting YOLOv12 to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e3f1435-5f3b-44cd-adc6-23ac8368e54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'yolov12' already exists and is not an empty directory.\n",
      "/mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/Ultimate-ONNX-for-Optimizing-Deep-Learning-Models/Chapter-8/yolov12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "# Clone YOLOv12 repository\n",
    "!git clone https://github.com/sunsmarterjie/yolov12.git\n",
    "%cd yolov12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32713554-ff53-4472-9cc7-7ee4c5b5a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are the minimal requirements as we don\n",
    "req = \"\"\"\n",
    "--extra-index-url https://download.pytorch.org/whl/cpu\n",
    "torch==2.2.2\n",
    "torchvision==0.17.2\n",
    "onnx==1.14.0\n",
    "PyYAML==6.0.1\n",
    "# scipy==1.13.0\n",
    "onnxslim==0.1.31\n",
    "onnxruntime==1.18.0\n",
    "opencv-python==4.9.0.80\n",
    "# psutil==5.9.8\n",
    "# py-cpuinfo==9.0.0\n",
    "huggingface-hub==0.23.2\n",
    "safetensors==0.4.3\n",
    "numpy==1.26.4\n",
    "matplotlib\"\"\"\n",
    "\n",
    "with open(\"requirements_minimul.txt\", \"w\") as f:\n",
    "    f.write(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5da06a8-6d4a-49fc-8019-c27533a5f2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch==2.2.2 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from -r requirements_minimul.txt (line 3)) (2.2.2+cpu)\n",
      "Requirement already satisfied: torchvision==0.17.2 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from -r requirements_minimul.txt (line 4)) (0.17.2+cpu)\n",
      "Requirement already satisfied: onnx==1.14.0 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from -r requirements_minimul.txt (line 5)) (1.14.0)\n",
      "Requirement already satisfied: PyYAML==6.0.1 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from -r requirements_minimul.txt (line 6)) (6.0.1)\n",
      "Requirement already satisfied: onnxslim==0.1.31 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from -r requirements_minimul.txt (line 8)) (0.1.31)\n",
      "Requirement already satisfied: onnxruntime==1.18.0 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from -r requirements_minimul.txt (line 9)) (1.18.0)\n",
      "Requirement already satisfied: opencv-python==4.9.0.80 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from -r requirements_minimul.txt (line 10)) (4.9.0.80)\n",
      "Requirement already satisfied: huggingface-hub==0.23.2 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from -r requirements_minimul.txt (line 13)) (0.23.2)\n",
      "Requirement already satisfied: safetensors==0.4.3 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from -r requirements_minimul.txt (line 14)) (0.4.3)\n",
      "Requirement already satisfied: numpy==1.26.4 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from -r requirements_minimul.txt (line 15)) (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from -r requirements_minimul.txt (line 16)) (3.10.1)\n",
      "Requirement already satisfied: sympy in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from torch==2.2.2->-r requirements_minimul.txt (line 3)) (1.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from torch==2.2.2->-r requirements_minimul.txt (line 3)) (4.13.2)\n",
      "Requirement already satisfied: fsspec in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from torch==2.2.2->-r requirements_minimul.txt (line 3)) (2025.3.2)\n",
      "Requirement already satisfied: filelock in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from torch==2.2.2->-r requirements_minimul.txt (line 3)) (3.18.0)\n",
      "Requirement already satisfied: networkx in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from torch==2.2.2->-r requirements_minimul.txt (line 3)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from torch==2.2.2->-r requirements_minimul.txt (line 3)) (3.1.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from torchvision==0.17.2->-r requirements_minimul.txt (line 4)) (11.2.1)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from onnx==1.14.0->-r requirements_minimul.txt (line 5)) (6.30.2)\n",
      "Requirement already satisfied: packaging in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from onnxslim==0.1.31->-r requirements_minimul.txt (line 8)) (25.0)\n",
      "Requirement already satisfied: flatbuffers in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from onnxruntime==1.18.0->-r requirements_minimul.txt (line 9)) (25.2.10)\n",
      "Requirement already satisfied: coloredlogs in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from onnxruntime==1.18.0->-r requirements_minimul.txt (line 9)) (15.0.1)\n",
      "Requirement already satisfied: requests in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from huggingface-hub==0.23.2->-r requirements_minimul.txt (line 13)) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from huggingface-hub==0.23.2->-r requirements_minimul.txt (line 13)) (4.67.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from matplotlib->-r requirements_minimul.txt (line 16)) (4.57.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from matplotlib->-r requirements_minimul.txt (line 16)) (1.3.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from matplotlib->-r requirements_minimul.txt (line 16)) (1.4.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from matplotlib->-r requirements_minimul.txt (line 16)) (2.9.0.post0)\n",
      "Requirement already satisfied: cycler>=0.10 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from matplotlib->-r requirements_minimul.txt (line 16)) (0.12.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from matplotlib->-r requirements_minimul.txt (line 16)) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->-r requirements_minimul.txt (line 16)) (1.17.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from coloredlogs->onnxruntime==1.18.0->-r requirements_minimul.txt (line 9)) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from jinja2->torch==2.2.2->-r requirements_minimul.txt (line 3)) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from requests->huggingface-hub==0.23.2->-r requirements_minimul.txt (line 13)) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from requests->huggingface-hub==0.23.2->-r requirements_minimul.txt (line 13)) (2025.1.31)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from requests->huggingface-hub==0.23.2->-r requirements_minimul.txt (line 13)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from requests->huggingface-hub==0.23.2->-r requirements_minimul.txt (line 13)) (2.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (from sympy->torch==2.2.2->-r requirements_minimul.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: thop in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages (0.1.1.post2209072238)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements_minimul.txt\n",
    "!pip install thop --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4385c06f-4ee7-4c99-b136-c8890c034ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention is not available on this device. Using scaled_dot_product_attention instead.\n",
      "Downloading https://github.com/sunsmarterjie/yolov12/releases/download/turbo/yolov12n.pt to 'yolov12n.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.26M/5.26M [00:00<00:00, 14.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ 'source' is missing. Using 'source=/mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/Ultimate-ONNX-for-Optimizing-Deep-Learning-Models/Chapter-8/yolov12/ultralytics/assets'.\n",
      "\n",
      "image 1/2 /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/Ultimate-ONNX-for-Optimizing-Deep-Learning-Models/Chapter-8/yolov12/ultralytics/assets/bus.jpg: 640x480 4 persons, 1 bus, 234.6ms\n",
      "image 2/2 /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/Ultimate-ONNX-for-Optimizing-Deep-Learning-Models/Chapter-8/yolov12/ultralytics/assets/zidane.jpg: 384x640 2 persons, 1 tie, 124.8ms\n",
      "Speed: 6.6ms preprocess, 179.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "# Run YOLOv12 model in Pytorch first.\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov12n.pt')\n",
    "res = model.predict()\n",
    "\n",
    "for i, r in enumerate(res):\n",
    "    r.save(f\"res_{i}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbe9897c-049a-4a28-973a-3973405474f6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "def display_side_by_side(img_path1, img_path2, caption1=\"Image 1\", caption2=\"Image 2\", \n",
    "                        width=512, title=\"Image Comparison\"):\n",
    "    \"\"\"\n",
    "    Display two images side by side with centered captions and a title.\n",
    "    \n",
    "    Parameters:\n",
    "    - img_path1: Path to the first image\n",
    "    - img_path2: Path to the second image\n",
    "    - caption1: Caption for the first image\n",
    "    - caption2: Caption for the second image\n",
    "    - width: Display width for each image in pixels\n",
    "    - title: Title for the entire comparison\n",
    "    \"\"\"\n",
    "    html = f\"\"\"\n",
    "    <div style=\"text-align: center; margin: 20px 0;\">\n",
    "        <h3>{title}</h3>\n",
    "        <table style=\"margin: 0 auto; border-collapse: collapse;\">\n",
    "            <tr>\n",
    "                <td style=\"padding: 10px; text-align: center; vertical-align: top;\">\n",
    "                    <div style=\"width: {width}px; margin: 0 auto;\">\n",
    "                        <img src=\"{img_path1}\" alt=\"{caption1}\" style=\"max-width: 100%; height: auto; display: block; margin: 0 auto;\"/>\n",
    "                        <p style=\"text-align: center; margin-top: 8px; font-weight: bold;\">{caption1}</p>\n",
    "                    </div>\n",
    "                </td>\n",
    "                <td style=\"padding: 10px; text-align: center; vertical-align: top;\">\n",
    "                    <div style=\"width: {width}px; margin: 0 auto;\">\n",
    "                        <img src=\"{img_path2}\" alt=\"{caption2}\" style=\"max-width: 100%; height: auto; display: block; margin: 0 auto;\"/>\n",
    "                        <p style=\"text-align: center; margin-top: 8px; font-weight: bold;\">{caption2}</p>\n",
    "                    </div>\n",
    "                </td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41a35645-305c-49c6-9d50-1fa9e7fc9930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"text-align: center; margin: 20px 0;\">\n",
       "        <h3>Pytorch Outputs</h3>\n",
       "        <table style=\"margin: 0 auto; border-collapse: collapse;\">\n",
       "            <tr>\n",
       "                <td style=\"padding: 10px; text-align: center; vertical-align: top;\">\n",
       "                    <div style=\"width: 512px; margin: 0 auto;\">\n",
       "                        <img src=\"./yolov12/ultralytics/assets/zidane.jpg\" alt=\"Original Image\" style=\"max-width: 100%; height: auto; display: block; margin: 0 auto;\"/>\n",
       "                        <p style=\"text-align: center; margin-top: 8px; font-weight: bold;\">Original Image</p>\n",
       "                    </div>\n",
       "                </td>\n",
       "                <td style=\"padding: 10px; text-align: center; vertical-align: top;\">\n",
       "                    <div style=\"width: 512px; margin: 0 auto;\">\n",
       "                        <img src=\"./yolov12/res_1.jpg\" alt=\"Image with detections\" style=\"max-width: 100%; height: auto; display: block; margin: 0 auto;\"/>\n",
       "                        <p style=\"text-align: center; margin-top: 8px; font-weight: bold;\">Image with detections</p>\n",
       "                    </div>\n",
       "                </td>\n",
       "            </tr>\n",
       "        </table>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_side_by_side(\n",
    "    \"./yolov12/ultralytics/assets/zidane.jpg\",\n",
    "    \"./yolov12/res_1.jpg\",\n",
    "    \"Original Image\",\n",
    "    \"Image with detections\",\n",
    "    title=\"Pytorch Outputs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "349c96a6-9d5a-4768-8e7f-041c2472c624",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.63 ðŸš€ Python-3.10.12 torch-2.2.2+cpu CPU (AMD Ryzen 5 5600H with Radeon Graphics)\n",
      "YOLOv12n summary (fused): 376 layers, 2,542,440 parameters, 0 gradients, 6.0 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov12n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (5.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.0 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.31...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 9.5s, saved as 'yolov12n.onnx' (10.0 MB)\n",
      "\n",
      "Export complete (10.4s)\n",
      "Results saved to \u001b[1m/mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/Ultimate-ONNX-for-Optimizing-Deep-Learning-Models/Chapter-8/yolov12\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolov12n.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolov12n.onnx imgsz=640 data=None  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yolov12n.onnx'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export to ONNX\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov12n.pt')\n",
    "model.export(format=\"onnx\", device=\"cpu\", simplify=True, nms=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349cc78c-ccb4-4b7d-bde9-3eed2e778f99",
   "metadata": {},
   "source": [
    "### Step-2 Execute FP32 model using ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2f071b-b359-4d9b-84e0-fdcd95bd5645",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bceadefa-bbbd-4991-8b8e-dc483911c2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 0.128s\n",
      "Saved results to output_fp32.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from yolo_helper.inference import YOLOInference\n",
    "from yolo_helper.visualization import draw_detections\n",
    "from yolo_helper.postprocessing import get_class_names\n",
    "\n",
    "# Configuration\n",
    "MODEL_PATH = \"yolov12/yolov12n.onnx\"\n",
    "IMAGE_PATH = \"./yolov12/ultralytics/assets/zidane.jpg\"\n",
    "CLASS_NAMES = get_class_names()  # COCO class names\n",
    "OUTPUT_PATH = \"output_fp32.jpg\"\n",
    "\n",
    "# Initialize detector\n",
    "detector = YOLOInference(MODEL_PATH)\n",
    "\n",
    "# Run inference\n",
    "result = detector(IMAGE_PATH, conf_thresh=0.25, iou_thresh=0.45)\n",
    "\n",
    "if result is not None:\n",
    "    image, detections = result\n",
    "    \n",
    "    # Draw and save results\n",
    "    output_image = draw_detections(image, detections, CLASS_NAMES)\n",
    "    cv2.imwrite(OUTPUT_PATH, output_image)\n",
    "    print(f\"Saved results to {OUTPUT_PATH}\")\n",
    "else:\n",
    "    print(\"No detections found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4e946f-9cc0-4e5c-91e6-08ac7779af66",
   "metadata": {},
   "source": [
    "### Step-3 Apply Static Quantization to model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e852153-bbdd-4a4c-a3b8-c30f4da52c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val2017.zip already exists - skipping download\n",
      "val2017 directory already exists - skipping extraction\n"
     ]
    }
   ],
   "source": [
    "# Download coco-val2017 dataset for calibration.\n",
    "\n",
    "command = \"\"\"\n",
    "# Create datasets directory if it doesn't exist\n",
    "mkdir -p ./datasets\n",
    "\n",
    "# Download coco-val2017 dataset only if zip file doesn't exist\n",
    "if [ ! -f ./datasets/val2017.zip ]; then\n",
    "    wget http://images.cocodataset.org/zips/val2017.zip -O ./datasets/val2017.zip\n",
    "else\n",
    "    echo \"val2017.zip already exists - skipping download\"\n",
    "fi\n",
    "\n",
    "# Extract only if the directory doesn't exist\n",
    "if [ ! -d ./datasets/val2017 ]; then\n",
    "    unzip ./datasets/val2017.zip -d ./datasets/\n",
    "else\n",
    "    echo \"val2017 directory already exists - skipping extraction\"\n",
    "fi\n",
    "\"\"\"\n",
    "\n",
    "with open(\"coco_val_2017_download.sh\", \"w\") as f:\n",
    "    f.writelines(command)\n",
    "    \n",
    "!./coco_val_2017_download.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "880efb57-39a3-4a9f-a00a-03b6479d199b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic Quantized model saved at: ./yolov12/yolov12n_int8_dynamic_quant.onnx\n",
      "Static Quantized model saved at: ./yolov12/yolov12n_int8_static_quant.onnx\n",
      "FP32 Model Size: 10.02 MB\n",
      "Dynamic Quantized Model Size: 2.96 MB\n",
      "Static Quantized Model Size: 3.12 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from yolo_helper.dataloader import COCODataset, YoloDataReader\n",
    "from onnxruntime.quantization import quantize_static, quantize_dynamic, QuantType, CalibrationMethod\n",
    "from onnxruntime.quantization.shape_inference import quant_pre_process\n",
    "\n",
    "dataset = COCODataset(\"./datasets/val2017\", sample=100)\n",
    "data_reader = YoloDataReader(dataset)\n",
    "\n",
    "fp32_path = \"./yolov12/yolov12n.onnx\"\n",
    "fp32_path_preproc = \"./yolov12/yolov12n_preproc.onnx\"\n",
    "int8_path_dynamic_quant = \"./yolov12/yolov12n_int8_dynamic_quant.onnx\"\n",
    "int8_path_static_quant = \"./yolov12/yolov12n_int8_static_quant.onnx\"\n",
    "\n",
    "# Firstly, apply shape inference and onnxruntime model optimization before quantizing the model.\n",
    "quant_pre_process(fp32_path, fp32_path_preproc, skip_symbolic_shape=True)\n",
    "\n",
    "# Apply dynamic quantization\n",
    "quantized_model = quantize_dynamic(\n",
    "    model_input=fp32_path_preproc,        # Input ONNX model\n",
    "    model_output=int8_path_dynamic_quant,       # Output quantized model\n",
    "    weight_type=QuantType.QInt8          # Quantize only weights to int8, activations will be quantize during runtime\n",
    ")\n",
    "\n",
    "print(f\"Dynamic Quantized model saved at: {int8_path_dynamic_quant}\")\n",
    "\n",
    "\n",
    "# Apply static quantization\n",
    "quantize_static(\n",
    "    model_input=fp32_path_preproc,          # Input ONNX model\n",
    "    model_output=int8_path_static_quant,    # Output quantized model\n",
    "    calibration_data_reader=data_reader,\n",
    "    weight_type=QuantType.QInt8,            # Quantize weights to int8\n",
    "    activation_type=QuantType.QInt8,        # Quantize activations to int8\n",
    "    calibrate_method=CalibrationMethod.MinMax,\n",
    "    extra_options={\"CalibStridedMinMax\": 4}    # Process 4 images at a time for calibration\n",
    ")\n",
    "\n",
    "print(f\"Static Quantized model saved at: {int8_path_static_quant}\")\n",
    "\n",
    "# Compare model sizes\n",
    "fp32_size = os.path.getsize(fp32_path) / 1024 / 1024\n",
    "dynamic_quant_size = os.path.getsize(int8_path_dynamic_quant) / 1024 / 1024\n",
    "static_quant_size = os.path.getsize(int8_path_static_quant) / 1024 / 1024\n",
    "\n",
    "print(f\"FP32 Model Size: {fp32_size:.2f} MB\")\n",
    "print(f\"Dynamic Quantized Model Size: {dynamic_quant_size:.2f} MB\")\n",
    "print(f\"Static Quantized Model Size: {static_quant_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56547780-9822-4662-a916-cbf3ca2d01cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 0.239s\n",
      "No detections found\n"
     ]
    }
   ],
   "source": [
    "# Run Static Quantized Model\n",
    "\n",
    "# Configuration\n",
    "MODEL_PATH = int8_path_static_quant\n",
    "IMAGE_PATH = \"./yolov12/ultralytics/assets/zidane.jpg\"\n",
    "CLASS_NAMES = get_class_names()  # COCO class names\n",
    "OUTPUT_PATH = \"./output_int8_static.jpg\"\n",
    "\n",
    "# Initialize detector\n",
    "detector = YOLOInference(MODEL_PATH)\n",
    "\n",
    "# Run inference with reduced conf_threshold.\n",
    "result = detector(IMAGE_PATH, conf_thresh=0.05, iou_thresh=0.45)\n",
    "\n",
    "if result is not None:\n",
    "    image, detections = result\n",
    "    \n",
    "    # Draw and save results\n",
    "    output_image = draw_detections(image, detections, CLASS_NAMES)\n",
    "    cv2.imwrite(OUTPUT_PATH, output_image)\n",
    "    print(f\"Saved results to {OUTPUT_PATH}\")\n",
    "else:\n",
    "    print(\"No detections found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5efb03f-c1c8-4871-8812-8a3eefe589b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplemented",
     "evalue": "[ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for ConvInteger(10) node with name '/model.0/conv/Conv_quant'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplemented\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m OUTPUT_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./output_int8_dynamic.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Initialize detector\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m detector \u001b[38;5;241m=\u001b[39m \u001b[43mYOLOInference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Run inference\u001b[39;00m\n\u001b[1;32m     13\u001b[0m result \u001b[38;5;241m=\u001b[39m detector(IMAGE_PATH, conf_thresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m, iou_thresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.45\u001b[39m)\n",
      "File \u001b[0;32m/mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/Ultimate-ONNX-for-Optimizing-Deep-Learning-Models/Chapter-8/yolo_helper/inference.py:20\u001b[0m, in \u001b[0;36mYOLOInference.__init__\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    Initialize YOLO inference session.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m        model_path: Path to ONNX model\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession \u001b[38;5;241m=\u001b[39m \u001b[43monnxruntime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mget_inputs()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[0;32m/mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:419\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m disabled_optimizers \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_inference_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisabled_optimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "File \u001b[0;32m/mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/yolo_env/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:483\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    480\u001b[0m     disabled_optimizers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(disabled_optimizers)\n\u001b[1;32m    482\u001b[0m \u001b[38;5;66;03m# initialize the C++ InferenceSession\u001b[39;00m\n\u001b[0;32m--> 483\u001b[0m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisabled_optimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess \u001b[38;5;241m=\u001b[39m sess\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess_options \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess\u001b[38;5;241m.\u001b[39msession_options\n",
      "\u001b[0;31mNotImplemented\u001b[0m: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for ConvInteger(10) node with name '/model.0/conv/Conv_quant'"
     ]
    }
   ],
   "source": [
    "# Run Dynamic Quantized Model\n",
    "\n",
    "# Configuration\n",
    "MODEL_PATH = int8_path_dynamic_quant\n",
    "IMAGE_PATH = \"./yolov12/ultralytics/assets/zidane.jpg\"\n",
    "CLASS_NAMES = get_class_names()  # COCO class names\n",
    "OUTPUT_PATH = \"./output_int8_dynamic.jpg\"\n",
    "\n",
    "# Initialize detector\n",
    "detector = YOLOInference(MODEL_PATH)\n",
    "\n",
    "# Run inference\n",
    "result = detector(IMAGE_PATH, conf_thresh=0.25, iou_thresh=0.45)\n",
    "\n",
    "if result is not None:\n",
    "    image, detections = result\n",
    "    \n",
    "    # Draw and save results\n",
    "    output_image = draw_detections(image, detections, CLASS_NAMES)\n",
    "    cv2.imwrite(OUTPUT_PATH, output_image)\n",
    "    print(f\"Saved results to {OUTPUT_PATH}\")\n",
    "else:\n",
    "    print(\"No detections found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f52ad88-2654-46b8-8678-f92a8bb4cbe3",
   "metadata": {},
   "source": [
    "### Step-4 Debug Static Quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec1adefd-99bb-4ace-a6c5-1052843a8410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Static Quantized model saved at: ./yolov12/yolov12n_int8_static_quant_w_concat_fp32.onnx\n"
     ]
    }
   ],
   "source": [
    "int8_path_static_quant_w_concat_fp32 = \"./yolov12/yolov12n_int8_static_quant_w_concat_fp32.onnx\"\n",
    "\n",
    "# Rewind the datareader so that it can be used again.\n",
    "data_reader.rewind()\n",
    "\n",
    "# Add last Concat node which concatenates boxes prediction and scores prediction.\n",
    "# Boxes prediction is in [0-640] range, while scores prediction is in [0-1] range. \n",
    "# This makes it difficult to quantize. Hence, keeping this node in FP32 precision\n",
    "# recovers the lost accuracy.\n",
    "quantize_static(\n",
    "    model_input=fp32_path_preproc,          # Input ONNX model\n",
    "    model_output=int8_path_static_quant_w_concat_fp32,    # Output quantized model\n",
    "    calibration_data_reader=data_reader,\n",
    "    weight_type=QuantType.QInt8,            # Quantize weights to int8\n",
    "    activation_type=QuantType.QInt8,        # Quantize activations to int8\n",
    "    calibrate_method=CalibrationMethod.MinMax,\n",
    "    nodes_to_exclude=[\"/model.21/Concat_5\"],\n",
    "    extra_options={\"CalibStridedMinMax\": 4}    # Process 4 images at a time for calibration\n",
    ")\n",
    "\n",
    "print(f\"Updated Static Quantized model saved at: {int8_path_static_quant_w_concat_fp32}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24adb7ae-4718-4d96-bc55-0ebbaa161a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 0.382s\n",
      "Saved results to ./output_int8_static_w_concat_fp32.jpg\n"
     ]
    }
   ],
   "source": [
    "# Run Updated Static Quantized Model\n",
    "\n",
    "# Configuration\n",
    "MODEL_PATH = int8_path_static_quant_w_concat_fp32\n",
    "IMAGE_PATH = \"./yolov12/ultralytics/assets/zidane.jpg\"\n",
    "CLASS_NAMES = get_class_names()  # COCO class names\n",
    "OUTPUT_PATH = \"./output_int8_static_w_concat_fp32.jpg\"\n",
    "\n",
    "# Initialize detector\n",
    "detector = YOLOInference(MODEL_PATH)\n",
    "\n",
    "# Run inference with 0.25 conf_threshold\n",
    "result = detector(IMAGE_PATH, conf_thresh=0.25, iou_thresh=0.45)\n",
    "\n",
    "if result is not None:\n",
    "    image, detections = result\n",
    "    \n",
    "    # Draw and save results\n",
    "    output_image = draw_detections(image, detections, CLASS_NAMES)\n",
    "    cv2.imwrite(OUTPUT_PATH, output_image)\n",
    "    print(f\"Saved results to {OUTPUT_PATH}\")\n",
    "else:\n",
    "    print(\"No detections found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1e2cc6f-6af7-4a8d-ab7c-3fe32606e90e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style=\"text-align: center; margin: 20px 0;\">\n",
       "        <h3>ONNX Runtime Static Quantized Outputs</h3>\n",
       "        <table style=\"margin: 0 auto; border-collapse: collapse;\">\n",
       "            <tr>\n",
       "                <td style=\"padding: 10px; text-align: center; vertical-align: top;\">\n",
       "                    <div style=\"width: 512px; margin: 0 auto;\">\n",
       "                        <img src=\"./yolov12/ultralytics/assets/zidane.jpg\" alt=\"Original Image\" style=\"max-width: 100%; height: auto; display: block; margin: 0 auto;\"/>\n",
       "                        <p style=\"text-align: center; margin-top: 8px; font-weight: bold;\">Original Image</p>\n",
       "                    </div>\n",
       "                </td>\n",
       "                <td style=\"padding: 10px; text-align: center; vertical-align: top;\">\n",
       "                    <div style=\"width: 512px; margin: 0 auto;\">\n",
       "                        <img src=\"./output_int8_static_w_concat_fp32.jpg\" alt=\"Image with detections\" style=\"max-width: 100%; height: auto; display: block; margin: 0 auto;\"/>\n",
       "                        <p style=\"text-align: center; margin-top: 8px; font-weight: bold;\">Image with detections</p>\n",
       "                    </div>\n",
       "                </td>\n",
       "            </tr>\n",
       "        </table>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage:\n",
    "display_side_by_side(\n",
    "    \"./yolov12/ultralytics/assets/zidane.jpg\",\n",
    "    \"./output_int8_static_w_concat_fp32.jpg\",\n",
    "    \"Original Image\",\n",
    "    \"Image with detections\",\n",
    "    title=\"ONNX Runtime Static Quantized Outputs\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
