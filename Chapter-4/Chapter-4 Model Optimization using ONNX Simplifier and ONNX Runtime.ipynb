{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fb3a7a8-d2e1-41a3-85fc-28f386ca7150",
   "metadata": {
    "id": "9fb3a7a8-d2e1-41a3-85fc-28f386ca7150"
   },
   "source": [
    "# Chapter-4 Model Optimization using ONNX Simplifier and ONNX Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f315b506-aa35-4839-bcc0-53f715ffcd42",
   "metadata": {
    "id": "f315b506-aa35-4839-bcc0-53f715ffcd42"
   },
   "source": [
    "#### In this notebook, we will try to optimize GPT2 ONNX model using ONNX Simplifier and ONNX Runtime. We will also see the impact of these optimizations on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32d17e8-7534-4352-b357-2a28e39f0534",
   "metadata": {
    "id": "b32d17e8-7534-4352-b357-2a28e39f0534"
   },
   "source": [
    "## Part-1 : Export GPT2 ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ZOA35IdE8yj1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZOA35IdE8yj1",
    "outputId": "d4d1f563-1994-400f-fdf0-70b1af39dab5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx==1.18.0 in /usr/local/lib/python3.11/dist-packages (1.18.0)\n",
      "Requirement already satisfied: onnxruntime==1.22.0 in /usr/local/lib/python3.11/dist-packages (1.22.0)\n",
      "Requirement already satisfied: onnx-simplifier==0.4.36 in /usr/local/lib/python3.11/dist-packages (0.4.36)\n",
      "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx==1.18.0) (2.0.2)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx==1.18.0) (5.29.5)\n",
      "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx==1.18.0) (4.14.1)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime==1.22.0) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime==1.22.0) (25.2.10)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime==1.22.0) (24.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime==1.22.0) (1.13.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from onnx-simplifier==0.4.36) (13.9.4)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime==1.22.0) (10.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->onnx-simplifier==0.4.36) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->onnx-simplifier==0.4.36) (2.19.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime==1.22.0) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->onnx-simplifier==0.4.36) (0.1.2)\n",
      "Requirement already satisfied: netron==8.4.3 in /usr/local/lib/python3.11/dist-packages (8.4.3)\n",
      "Requirement already satisfied: transformers==4.53.2 in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (0.33.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.53.2) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.2) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.2) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.53.2) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.2) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.53.2) (2025.7.9)\n"
     ]
    }
   ],
   "source": [
    "# Install prerequisites\n",
    "!pip install onnx==1.18.0 onnxruntime==1.22.0 onnx-simplifier==0.4.36\n",
    "!pip install netron==8.4.3 transformers==4.53.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2033289e-2d0b-4c0b-a867-2b01f903e5d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2033289e-2d0b-4c0b-a867-2b01f903e5d9",
    "outputId": "4a56a920-b38e-4332-ad94-6d799973e556"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Ids shape:  torch.Size([1, 4])\n",
      "------------------------------------------------------------\n",
      "Given input: Once upon a time\n",
      "Generated output: Once upon a time, the world was a place of great beauty and great danger. The world of the gods was the place where the great gods were born, and where they were to live.\n",
      "\n",
      "The world that was created was not the same\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load GPT2 model from HuggingFace: https://huggingface.co/openai-community/gpt2\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "# Encode the input text (prompt) into tokens\n",
    "input_text = \"Once upon a time\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "print(\"Input Ids shape: \", input_ids.shape)\n",
    "\n",
    "# Generate text using the model\n",
    "output = model.generate(input_ids, max_length=50, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "\n",
    "# Decode the generated tokens back into text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"--\"*30)\n",
    "print(f\"Given input: {input_text}\")\n",
    "print(f\"Generated output: {generated_text}\")\n",
    "print(\"--\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea812f09-f0bc-4501-82b9-7b3631aa325d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ea812f09-f0bc-4501-82b9-7b3631aa325d",
    "outputId": "68e562d9-87df-49e0-97a1-750b59551ac3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with static shapes successfully exported to ./exported_models/gpt2_hf_static_shape.onnx\n",
      "Model with dynamic shapes successfully exported to ./exported_models/gpt2_hf_dynamic_shape.onnx\n"
     ]
    }
   ],
   "source": [
    "# Export GPT2 model to ONNX\n",
    "\n",
    "import os\n",
    "import torch\n",
    "os.makedirs(\"./exported_models/\", exist_ok=True)\n",
    "static_shape_output_path = \"./exported_models/gpt2_hf_static_shape.onnx\"\n",
    "dynamic_shape_output_path = \"./exported_models/gpt2_hf_dynamic_shape.onnx\"\n",
    "\n",
    "# Export the model to ONNX with static shapes\n",
    "dummy_static_input_ids = torch.ones([1, 128], dtype=torch.int32)\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    args=(dummy_static_input_ids,),\n",
    "    f=static_shape_output_path,\n",
    "    kwargs={'logits_to_keep': None},\n",
    "    input_names=[\"input_ids\"],\n",
    "    output_names=[\"logits\"],\n",
    "    opset_version=14\n",
    ")\n",
    "\n",
    "# Export the model to ONNX with dynamic shapes\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    args=(input_ids,),\n",
    "    f=dynamic_shape_output_path,\n",
    "    kwargs={'logits_to_keep': None},\n",
    "    input_names=[\"input_ids\"],\n",
    "    output_names=[\"logits\"],\n",
    "    # Dynamic axes for batch size and sequence length\n",
    "    dynamic_axes={\"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},  \n",
    "                  \"logits\": {0: \"batch_size\", 1: \"sequence_length\"}},\n",
    "    opset_version=14\n",
    ")\n",
    "\n",
    "print(f\"Model with static shapes successfully exported to {static_shape_output_path}\")\n",
    "print(f\"Model with dynamic shapes successfully exported to {dynamic_shape_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a782dd92-fa29-4f55-9632-b5acb8cd4968",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "a782dd92-fa29-4f55-9632-b5acb8cd4968",
    "outputId": "585d1832-7eb4-4b67-fcb0-19d65b2d0248"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"500\"\n",
       "            src=\"http://localhost:6006\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7992d4213e50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the exported model with static shapes\n",
    "\n",
    "import IPython\n",
    "import netron\n",
    "\n",
    "port = 6006\n",
    "netron.start(static_shape_output_path, port, browse=False)\n",
    "IPython.display.IFrame(f\"http://localhost:{port}\", width=1000, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "VhMvZCqU9OlP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "VhMvZCqU9OlP",
    "outputId": "c51256b2-1d7c-4d20-9c81-08dde675f140"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"500\"\n",
       "            src=\"http://localhost:6006\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7992d4211e90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the exported model with dynamic shapes\n",
    "\n",
    "import IPython\n",
    "import netron\n",
    "\n",
    "port = 6006\n",
    "netron.start(dynamic_shape_output_path, port, browse=False)\n",
    "IPython.display.IFrame(f\"http://localhost:{port}\", width=1000, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "qJYFgnf5AaJq",
   "metadata": {
    "id": "qJYFgnf5AaJq"
   },
   "outputs": [],
   "source": [
    "# Below code is used to compare the original model with optimized model\n",
    "\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "def check_performance(model_path, input_data, num_iter=100):\n",
    "    # Perform inference and measure timing\n",
    "    session = ort.InferenceSession(model_path)\n",
    "    start = time.time()\n",
    "    for i in range(num_iter):\n",
    "        outputs = session.run(None, input_data)\n",
    "    end = time.time()\n",
    "\n",
    "    time_diff = (end-start)/num_iter\n",
    "    print(f\"Inference time: {time_diff:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336cb840-d9b4-48c5-a828-c7f0119a7314",
   "metadata": {
    "id": "336cb840-d9b4-48c5-a828-c7f0119a7314"
   },
   "source": [
    "## Part-2 : Optimize model using ONNX Simplifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac46f573-5d71-4428-aad5-fe9f30601e2c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac46f573-5d71-4428-aad5-fe9f30601e2c",
    "outputId": "d693ddb0-08a0-48fc-ab1b-628b65360bb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with static shapes:\n",
      "Before Nodes: 1168\n",
      "After Nodes: 673\n",
      "Model with dynamic shapes:\n",
      "Before Nodes: 2766\n",
      "After Nodes: 1435\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnxsim import simplify\n",
    "\n",
    "def optimize_model_using_simplifier(model_path, output_path):\n",
    "    # Load onnx model\n",
    "    onnx_model = onnx.load(model_path)\n",
    "\n",
    "    # Simplify model using ONNX Simplifier\n",
    "    simplified_model, status = simplify(onnx_model)\n",
    "\n",
    "    # Save simplified model\n",
    "    onnx.save(simplified_model, output_path)\n",
    "    print(f\"Before Nodes: {len(onnx_model.graph.node)}\")\n",
    "    print(f\"After Nodes: {len(simplified_model.graph.node)}\")\n",
    "\n",
    "print(\"Model with static shapes:\")\n",
    "opt_model_onnxsim_static_shape = \"./exported_models/gpt2_hf_static_shapes_onnxsim.onnx\"\n",
    "optimize_model_using_simplifier(static_shape_output_path, opt_model_onnxsim_static_shape)\n",
    "\n",
    "print(\"Model with dynamic shapes:\")\n",
    "opt_model_onnxsim_dynamic_shape = \"./exported_models/gpt2_hf_dynamic_shapes_onnxsim.onnx\"\n",
    "optimize_model_using_simplifier(dynamic_shape_output_path, opt_model_onnxsim_dynamic_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca0b1b0a-4d2b-4854-97b6-b12f8a38ac49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "ca0b1b0a-4d2b-4854-97b6-b12f8a38ac49",
    "outputId": "0550423f-d139-4395-b413-6322e1ea112c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"500\"\n",
       "            src=\"http://localhost:6006\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7992d41e8650>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the optimized model with static shapes\n",
    "\n",
    "import IPython\n",
    "import netron\n",
    "\n",
    "port = 6006\n",
    "netron.start(opt_model_onnxsim_static_shape, port, browse=False)\n",
    "IPython.display.IFrame(f\"http://localhost:{port}\", width=1000, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "K5eYj2Zl_QOj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "K5eYj2Zl_QOj",
    "outputId": "43947ebe-3e6f-44f6-f48f-5a96cabeb987"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"500\"\n",
       "            src=\"http://localhost:6006\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7992ba3cbc50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the optimized model with dynamic shapes\n",
    "\n",
    "import IPython\n",
    "import netron\n",
    "\n",
    "port = 6006\n",
    "netron.start(opt_model_onnxsim_dynamic_shape, port, browse=False)\n",
    "IPython.display.IFrame(f\"http://localhost:{port}\", width=1000, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5h5oY9jtBpoB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5h5oY9jtBpoB",
    "outputId": "02783eec-d8d8-4550-c9c0-d12209fdb44b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model with static shapes\n",
      "Inference time: 0.6316 seconds\n",
      "Optimized model with static shapes\n",
      "Inference time: 0.5367 seconds\n"
     ]
    }
   ],
   "source": [
    "# Let us make a dummy input tensor of shape [1, 128] for checking the performance of the models\n",
    "input_data_for_static_shape = {\"input_ids\" : np.random.randint(low=0, high=100, size=(1, 128), dtype=np.int32)}\n",
    "\n",
    "# Check performance for static shape model\n",
    "print(\"Original model with static shapes\")\n",
    "check_performance(static_shape_output_path, input_data_for_static_shape)\n",
    "\n",
    "print(\"Optimized model with static shapes\")\n",
    "check_performance(opt_model_onnxsim_static_shape, input_data_for_static_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "qj-MwYROFOBn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qj-MwYROFOBn",
    "outputId": "b3e58d7f-7f6c-48d1-c9bd-e60fd450c2c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model with dynamic shapes\n",
      "Inference time: 0.5671 seconds\n",
      "Optimized model with dynamic shapes\n",
      "Inference time: 0.5315 seconds\n"
     ]
    }
   ],
   "source": [
    "input_data_for_dynamic_shape = {\"input_ids\" : np.random.randint(low=0, high=100, size=(1, 128), dtype=np.int64)}\n",
    "\n",
    "# Check performance for dynamic shape model\n",
    "print(\"Original model with dynamic shapes\")\n",
    "check_performance(dynamic_shape_output_path, input_data_for_dynamic_shape)\n",
    "\n",
    "print(\"Optimized model with dynamic shapes\")\n",
    "check_performance(opt_model_onnxsim_dynamic_shape, input_data_for_dynamic_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851efd85-c39f-4c0f-bcf7-caf73fc510c0",
   "metadata": {
    "id": "851efd85-c39f-4c0f-bcf7-caf73fc510c0"
   },
   "source": [
    "## Part-3 : Optimize model using ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6MqwDXJ_UGO",
   "metadata": {
    "id": "f6MqwDXJ_UGO"
   },
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "\n",
    "def optimize_model_using_ort(model_path, output_path):\n",
    "    # Load onnx model\n",
    "    sess_options = rt.SessionOptions()\n",
    "    sess_options.graph_optimization_level = rt.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "    # Below are the different levels of optimizations in onnxruntime\n",
    "    # rt.GraphOptimizationLevel.ORT_DISABLE_ALL -> Disables all optimizations\n",
    "    # rt.GraphOptimizationLevel.ORT_ENABLE_BASIC -> Enables basic optimizations\n",
    "    # rt.GraphOptimizationLevel.ORT_ENABLE_EXTENDED -> Enables basic and extended optimizations\n",
    "    # rt.GraphOptimizationLevel.ORT_ENABLE_ALL -> Enables all available optimizations including layout optimizations\n",
    "\n",
    "    # To enable model serialization after graph optimization set this\n",
    "    sess_options.optimized_model_filepath = output_path\n",
    "\n",
    "    session = rt.InferenceSession(model_path, sess_options)\n",
    "\n",
    "    # No need to run the model. Initializing the session will generate the optimized model\n",
    "\n",
    "    orig_model = onnx.load(model_path)\n",
    "    opt_model = onnx.load(output_path)\n",
    "    print(f\"Before Nodes: {len(orig_model.graph.node)}\")\n",
    "    print(f\"After Nodes: {len(opt_model.graph.node)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "Vpk143mpSRIS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vpk143mpSRIS",
    "outputId": "f28da5c3-8c1e-4f4d-87c7-d6c61a1b91d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with static shapes:\n",
      "Before Nodes: 1168\n",
      "After Nodes: 365\n",
      "Model with dynamic shapes:\n",
      "Before Nodes: 2766\n",
      "After Nodes: 1120\n"
     ]
    }
   ],
   "source": [
    "print(\"Model with static shapes:\")\n",
    "opt_model_ort_static_shape = \"./exported_models/gpt2_hf_static_shapes_ort.onnx\"\n",
    "optimize_model_using_ort(static_shape_output_path, opt_model_ort_static_shape)\n",
    "\n",
    "print(\"Model with dynamic shapes:\")\n",
    "opt_model_ort_dynamic_shape = \"./exported_models/gpt2_hf_dynamic_shapes_ort.onnx\"\n",
    "optimize_model_using_ort(dynamic_shape_output_path, opt_model_ort_dynamic_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40e9cc4e-0004-42b7-af53-9209fb499c50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "40e9cc4e-0004-42b7-af53-9209fb499c50",
    "outputId": "63ae3efc-52b5-4fbb-80ef-feceab6be6fe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"500\"\n",
       "            src=\"http://localhost:6006\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7992ba3c87d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the optimized model with dynamic shapes\n",
    "\n",
    "import IPython\n",
    "import netron\n",
    "\n",
    "port = 6006\n",
    "netron.start(opt_model_ort_static_shape, port, browse=False)\n",
    "IPython.display.IFrame(f\"http://localhost:{port}\", width=1000, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7492eb42-0028-44e5-a5d9-e00428fafd43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "7492eb42-0028-44e5-a5d9-e00428fafd43",
    "outputId": "a18d3465-356b-4cb8-ba93-47a7ff73e9d2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"500\"\n",
       "            src=\"http://localhost:6006\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7992cd57ef10>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the optimized model with dynamic shapes\n",
    "\n",
    "import IPython\n",
    "import netron\n",
    "\n",
    "port = 6006\n",
    "netron.start(opt_model_ort_dynamic_shape, port, browse=False)\n",
    "IPython.display.IFrame(f\"http://localhost:{port}\", width=1000, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "64tPIzFpSLwY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64tPIzFpSLwY",
    "outputId": "16bd4c67-130e-409d-ddaa-6ff190c830b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model with static shapes\n",
      "Inference time: 0.5829 seconds\n",
      "Optimized model with static shapes\n",
      "Inference time: 0.5348 seconds\n"
     ]
    }
   ],
   "source": [
    "# Check performance for static shape model\n",
    "print(\"Original model with static shapes\")\n",
    "check_performance(static_shape_output_path, input_data_for_static_shape)\n",
    "\n",
    "print(\"Optimized model with static shapes\")\n",
    "check_performance(opt_model_ort_static_shape, input_data_for_static_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "WDIvF8C7UjIH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WDIvF8C7UjIH",
    "outputId": "babd55ab-22c8-4c8b-8001-94ae33253aad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model with dynamic shapes\n",
      "Inference time: 0.5383 seconds\n",
      "Optimized model with dynamic shapes\n",
      "Inference time: 0.5283 seconds\n"
     ]
    }
   ],
   "source": [
    "# Check performance for dynamic shape model\n",
    "print(\"Original model with dynamic shapes\")\n",
    "check_performance(dynamic_shape_output_path, input_data_for_dynamic_shape)\n",
    "\n",
    "print(\"Optimized model with dynamic shapes\")\n",
    "check_performance(opt_model_ort_dynamic_shape, input_data_for_dynamic_shape)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
