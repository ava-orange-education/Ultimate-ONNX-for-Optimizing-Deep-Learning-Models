{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2bfe63b-1c70-4e3e-b6c8-c9e0740befc2",
   "metadata": {},
   "source": [
    "# Chapter-10 End to End Execution of SmolLM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a496e34-d789-4cfe-9ab3-6db7cffea62a",
   "metadata": {},
   "source": [
    "#### In this notebook, we will use the concepts which we learned in the earlier chapters and try to play with SmolLM model in an end-to-end manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0f26b7-9383-4f93-bd6a-1cba18a0f693",
   "metadata": {},
   "source": [
    "### Step-1 Load and Run model in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "386b5428-6bcd-4e67-bc1d-77885317aef4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (3.4.0)\n",
      "Requirement already satisfied: onnx in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (1.14.0)\n",
      "Requirement already satisfied: onnxsim in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (0.4.36)\n",
      "Requirement already satisfied: onnxruntime in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (1.18.0)\n",
      "Requirement already satisfied: transformers in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (4.49.0)\n",
      "Requirement already satisfied: torch in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (2.2.2+cpu)\n",
      "Requirement already satisfied: filelock in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: aiohttp in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: requests>=2.32.2 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: packaging in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: xxhash in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from datasets) (0.31.2)\n",
      "Requirement already satisfied: pandas in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: fsspec[http]<=2024.12.0,>=2023.1.0 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from datasets) (2024.12.0)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from onnx) (4.12.2)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from onnx) (6.30.2)\n",
      "Requirement already satisfied: rich in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from onnxsim) (13.9.4)\n",
      "Requirement already satisfied: coloredlogs in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from onnxruntime) (25.2.10)\n",
      "Requirement already satisfied: sympy in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from onnxruntime) (1.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: networkx in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from aiohttp->datasets) (2.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from rich->onnxsim) (2.19.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from rich->onnxsim) (3.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->onnxsim) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# Install prerequisites\n",
    "!pip install datasets onnx onnxsim onnxruntime transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a09f8c1-3f3e-4d28-ac9f-05071dc8d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import onnx\n",
    "import torch\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from typing import Tuple, List\n",
    "from transformers.cache_utils import DynamicCache\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "from onnxruntime.quantization.shape_inference import quant_pre_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "101ea5d6-1d72-468f-9867-1034bf5d33f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "/mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.OnnxFunction.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "2025-05-25 22:32:21.545083: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748192542.742601   34630 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748192543.247153   34630 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-25 22:32:26.081159: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Input to the model =====\n",
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "<|im_start|>user\n",
      "What is gravity?<|im_end|>\n",
      "\n",
      "===== Model's prediction =====\n",
      "<|im_start|>assistant\n",
      "Gravity is a fundamental force of nature that attracts objects with mass towards each other. It is a result of the interaction between mass, energy, and space itself. In the context of our universe, gravity is a result of the curvature of spacetime caused by the presence of mass and energy.\n",
      "\n",
      "Imagine spacetime as a trampoline. When you place a heavy object, like a bowling ball, on the trampoline, it creates a depression in the surface. This depression is caused by the object's mass and the energy it contains. The more massive the object, the larger the depression.\n",
      "\n",
      "Now, when you move an object, such as a bowling ball, it creates a gravitational pull on the surrounding space. This gravitational pull is what causes the bowling ball to move towards the center of the trampoline. The more massive the object, the stronger the gravitational pull.\n",
      "\n",
      "Gravity is a universal force that acts between all objects in the universe, regardless of their mass or energy. It is a fundamental aspect of the universe and plays a crucial role in shaping the behavior of celestial bodies, including planets, stars, and galaxies.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "# Load the model from Huggingface and run it in Pytorch\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             attn_implementation=\"eager\", \n",
    "                                             use_cache=True)\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is gravity?\"}]\n",
    "input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=1000)\n",
    "\n",
    "# Decode only the new tokens\n",
    "input_length = inputs.input_ids.shape[1]\n",
    "output_text = tokenizer.decode(outputs[0][input_length:])\n",
    "                               \n",
    "print(\"===== Input to the model =====\")\n",
    "print(input_text)\n",
    "print(\"===== Model's prediction =====\")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07858c61-d9f0-4eb4-9aba-a9c63978434c",
   "metadata": {},
   "source": [
    "### Step-2 Export model to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d48d2a4-0df9-4a43-afc2-91d357f795eb",
   "metadata": {},
   "source": [
    "#### Export SmolLM model in two variants. One model is used for 1st inference and other model is used for all the subsequent inferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfb93c5-3b9c-4ad5-835e-f0e84dc6116f",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"./extras/Figure-10.3.png\" alt=\"\" width=\"800\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99e606f6-c373-4b83-9d9b-2420a20250f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:731: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder init model exported as ./smol_lm_init.onnx\n"
     ]
    }
   ],
   "source": [
    "# Export SmolLM Init model\n",
    "class SmolLMInit(torch.nn.Module):\n",
    "    \"\"\"Smol LM init model with lm head.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: torch.nn.Module, lm_head: torch.nn.Module):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lm_head = lm_head\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids: torch.Tensor, \n",
    "    ) -> Tuple[torch.Tensor, ...]:\n",
    "        \"\"\"Forward pass for decoder initialization.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Input token IDs for decoder\n",
    "            \n",
    "        Returns:\n",
    "            Tuple containing logits and past key values\n",
    "        \"\"\"\n",
    "        output = self.model(\n",
    "            input_ids=input_ids,\n",
    "            use_cache=True,\n",
    "        )\n",
    "        lm_logits = self.lm_head(output[0])\n",
    "        return (lm_logits,) + output.past_key_values.to_legacy_cache()\n",
    "\n",
    "def export_prefill(init_model_path, onnx_opset):\n",
    "    # Model configuration\n",
    "    batch = 1\n",
    "    seq_len = 16\n",
    "\n",
    "    # Create dummy inputs\n",
    "    input_ids = torch.randint(0, model.config.vocab_size, (batch, seq_len))    # [batch, seq]\n",
    "\n",
    "    # Initialize model\n",
    "    init_model = SmolLMInit(model.model, model.lm_head)\n",
    "\n",
    "    # Add inputs and outputs in dynamic axes\n",
    "    dynamic_axes = {\n",
    "        'input_ids': {0: 'batch', 1: 'sequence_length'},\n",
    "        'logits': {0: 'batch', 1: 'sequence_length'},\n",
    "    }\n",
    "    \n",
    "    # Prepare output names\n",
    "    init_input_names = [\"input_ids\"]\n",
    "    init_output_names = [\"logits\"]\n",
    "    for i in range(model.config.num_hidden_layers):\n",
    "        init_output_names.append(f'past_key_values.{i}.key')\n",
    "        init_output_names.append(f'past_key_values.{i}.value')\n",
    "        dynamic_axes[f'past_key_values.{i}.key'] = {0: 'batch', 2: 'sequence_length'}\n",
    "        dynamic_axes[f'past_key_values.{i}.value'] = {0: 'batch', 2: 'sequence_length'}\n",
    "\n",
    "    # Export to ONNX\n",
    "    torch.onnx.export(\n",
    "        init_model, (input_ids), init_model_path,\n",
    "        input_names=init_input_names, output_names=init_output_names,\n",
    "        dynamic_axes=dynamic_axes, opset_version=onnx_opset,\n",
    "    )\n",
    "\n",
    "    print(f\"Decoder init model exported as {init_model_path}\")\n",
    "\n",
    "\n",
    "init_model_path = \"./smol_lm_init.onnx\"\n",
    "onnx_opset = 15\n",
    "export_prefill(init_model_path, onnx_opset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7b85023-6a16-40ed-bc96-54408c29d30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages/transformers/cache_utils.py:451: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  or len(self.key_cache[layer_idx]) == 0  # the layer has no cache\n",
      "/mnt/d/Meet/Company/Orange Eva Publication/Jupyter Notebook/onnx_env/lib/python3.10/site-packages/transformers/cache_utils.py:435: TracerWarning: Using len to get tensor shape might cause the trace to be incorrect. Recommended usage would be tensor.shape[0]. Passing a tensor of different shape might lead to errors or silently give incorrect results.\n",
      "  len(self.key_cache[layer_idx]) == 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder step model exported as ./smol_lm_step.onnx\n"
     ]
    }
   ],
   "source": [
    "# Export SmolLM Step model\n",
    "class SmolLMStep(torch.nn.Module):\n",
    "    \"\"\"Smol LM step model with lm head.\"\"\"\n",
    "    \n",
    "    def __init__(self, model: torch.nn.Module, lm_head: torch.nn.Module):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.lm_head = lm_head\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        input_ids: torch.Tensor, \n",
    "        cache_position: torch.Tensor,\n",
    "        *past_key_values: Tuple[torch.Tensor, ...]\n",
    "    ) -> Tuple[torch.Tensor, ...]:\n",
    "        \"\"\"Forward pass for decoder initialization.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Input token IDs for decoder\n",
    "            cache_position: Position index for caching\n",
    "            *past_key_values: Past key-value pairs for attention\n",
    "            \n",
    "        Returns:\n",
    "            Tuple containing logits and updated key values\n",
    "        \"\"\"\n",
    "        # Reformat past_key_values into expected structure\n",
    "        num_layers = len(past_key_values) // 2\n",
    "        reformatted_past = []\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            layer_past = (\n",
    "                past_key_values[2 * i],     # self_key\n",
    "                past_key_values[2 * i + 1], # self_value\n",
    "            )\n",
    "            reformatted_past.append(layer_past)\n",
    "\n",
    "        dynamic_cache = DynamicCache.from_legacy_cache(reformatted_past, num_layers)\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            past_key_values=dynamic_cache,\n",
    "            use_cache=True,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "        \n",
    "        lm_logits = self.lm_head(outputs[0])\n",
    "        return (lm_logits,) + outputs.past_key_values.to_legacy_cache()\n",
    "\n",
    "\n",
    "def export_decode(step_model_path, onnx_opset):\n",
    "    def create_dummy_past_key_values(\n",
    "        batch: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        past_seq_len: int,\n",
    "        head_dim: int\n",
    "    ) -> List[torch.Tensor]:\n",
    "        \"\"\"Create dummy past key values for ONNX export.\n",
    "        \n",
    "        Args:\n",
    "            batch: Batch size\n",
    "            num_layers: Number of decoder layers\n",
    "            num_heads: Number of attention heads\n",
    "            past_seq_len: Length of past sequence\n",
    "            head_dim: Dimension of each attention head\n",
    "            \n",
    "        Returns:\n",
    "            List of dummy tensors for past key values\n",
    "        \"\"\"\n",
    "        dummy_kvs = []\n",
    "\n",
    "        self_attn_kv_shape = (batch, num_heads, past_seq_len, head_dim)\n",
    "        for _ in range(num_layers):\n",
    "            # Self attention keys/values\n",
    "            dummy_kvs.append(torch.rand(*self_attn_kv_shape))  # self_key\n",
    "            dummy_kvs.append(torch.rand(*self_attn_kv_shape))  # self_value\n",
    "        \n",
    "        return dummy_kvs\n",
    "\n",
    "\n",
    "    # Define input shape variables\n",
    "    past_seq_len = 16\n",
    "    step_seq_len = 1\n",
    "    batch = 1\n",
    "\n",
    "    # Create dummy inputs\n",
    "    dummy_input_ids = torch.randint(0, model.config.vocab_size, (batch, step_seq_len))\n",
    "    dummy_cache_position = torch.tensor([0], dtype=torch.long)\n",
    "    dummy_past_key_values = create_dummy_past_key_values(\n",
    "        batch, model.config.num_hidden_layers, model.config.num_key_value_heads, past_seq_len, model.config.head_dim\n",
    "    )\n",
    "    \n",
    "    # Prepare dynamic axes\n",
    "    dynamic_axes = {\n",
    "        \"input_ids\": {0: 'batch', 1: \"curr_seq_len\"},\n",
    "        \"logits\": {0: 'batch', 1: \"curr_seq_len\"},\n",
    "    }\n",
    "\n",
    "    # Prepare output names\n",
    "    step_output_names = [\"logits\"]\n",
    "    step_input_names = [\"input_ids\", \"cache_position\"]\n",
    "    past_kv_dynamic_axes = {0: 'batch', 2: \"past_seq\"}\n",
    "    present_kv_dynamic_axes = {0: 'batch', 2: \"past_seq_plus_curr_seq_len\"}\n",
    "    for i in range(model.config.num_hidden_layers):\n",
    "        dynamic_axes[f'past_key_values.{i}.key'] = past_kv_dynamic_axes\n",
    "        dynamic_axes[f'past_key_values.{i}.value'] = past_kv_dynamic_axes\n",
    "        dynamic_axes[f'present_key_values.{i}.key'] = present_kv_dynamic_axes\n",
    "        dynamic_axes[f'present_key_values.{i}.value'] = present_kv_dynamic_axes\n",
    "        step_input_names.append(f'past_key_values.{i}.key')\n",
    "        step_input_names.append(f'past_key_values.{i}.value')\n",
    "        step_output_names.append(f'present_key_values.{i}.key')\n",
    "        step_output_names.append(f'present_key_values.{i}.value')\n",
    "\n",
    "    # Initialize and export model\n",
    "    step_model = SmolLMStep(model.model, model.lm_head)\n",
    "\n",
    "    torch.onnx.export(\n",
    "        step_model,\n",
    "        (dummy_input_ids, dummy_cache_position, *dummy_past_key_values),\n",
    "        step_model_path,\n",
    "        input_names=step_input_names, output_names=step_output_names,\n",
    "        dynamic_axes=dynamic_axes, opset_version=onnx_opset,\n",
    "    )\n",
    "\n",
    "    print(f\"Decoder step model exported as {step_model_path}\")\n",
    "\n",
    "\n",
    "step_model_path = \"./smol_lm_step.onnx\"\n",
    "onnx_opset = 15\n",
    "export_decode(step_model_path, onnx_opset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ad3e705-1982-4e17-8c00-9c2854bdf496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mYour model contains \"Tile\" ops or/and \"ConstantOfShape\" ops. Folding these ops \u001b[0m\n",
      "\u001b[1;35mcan make the simplified model much larger. If it is not expected, please specify\u001b[0m\n",
      "\u001b[1;35m\"--no-large-tensor\" (which will lose some optimization chances)\u001b[0m\n",
      "Simplifying\u001b[33m...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mYour model contains \"Tile\" ops or/and \"ConstantOfShape\" ops. Folding these ops \u001b[0m\n",
      "\u001b[1;35mcan make the simplified model much larger. If it is not expected, please specify\u001b[0m\n",
      "\u001b[1;35m\"--no-large-tensor\" (which will lose some optimization chances)\u001b[0m\n",
      "Simplifying\u001b[33m...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Optimize models using ONNX Simplifier\n",
    "\n",
    "!python -m onnxsim smol_lm_init.onnx smol_lm_init.onnx\n",
    "!python -m onnxsim smol_lm_step.onnx smol_lm_step.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851a2fc6-3b37-4953-ba8e-464771d4a52e",
   "metadata": {},
   "source": [
    "### Step-3 Run both models using ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f2831af-8307-4d88-9e51-f1fe8056901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for inference\n",
    "def preprocess(user_text, tokenizer):\n",
    "    messages = [{\"role\": \"user\", \"content\": user_text}]\n",
    "    input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"np\").input_ids\n",
    "    return {\"input_ids\": input_ids}\n",
    "\n",
    "def run_models(init_model, step_model, inputs, tokenizer, max_iters=100):\n",
    "    past_seq_len = inputs[\"input_ids\"].shape[1]\n",
    "    print(\"Inputs shape: \", inputs['input_ids'].shape)\n",
    "    \n",
    "    # Run init model\n",
    "    outputs = init_model.run(None, inputs)\n",
    "    logits = outputs[0]\n",
    "    print(\"Logits shape: \", logits.shape)\n",
    "    generated_ids = [int(np.argmax(logits[0, -1]))]\n",
    "\n",
    "    step_inputs = {}\n",
    "    step_inputs[\"input_ids\"] = np.array(generated_ids[-1]).reshape(1, -1)\n",
    "    \n",
    "    num_layers = (len(outputs) - 1) // 2\n",
    "    for i in range(num_layers):\n",
    "        step_inputs[f'past_key_values.{i}.key'] = outputs[i * 2 + 1]\n",
    "        step_inputs[f'past_key_values.{i}.value'] = outputs[i * 2 + 2]\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        step_inputs[\"cache_position\"] = np.array([past_seq_len + i], dtype=np.int64)\n",
    "\n",
    "        # Run step model iteratively\n",
    "        step_outputs = step_model.run(None, step_inputs)\n",
    "        logits = step_outputs[0]\n",
    "        pred_token = int(np.argmax(logits[0, -1]))\n",
    "        \n",
    "        if pred_token == tokenizer.eos_token_id:\n",
    "            print(\"Stopping generateion as EOS token reached.\")\n",
    "            break\n",
    "\n",
    "        generated_ids.append(pred_token)\n",
    "\n",
    "        step_inputs[\"input_ids\"] = np.array(pred_token).reshape(1, -1)\n",
    "        for i in range(num_layers):\n",
    "            step_inputs[f'past_key_values.{i}.key'] = step_outputs[i * 2 + 1]\n",
    "            step_inputs[f'past_key_values.{i}.value'] = step_outputs[i * 2 + 2]\n",
    "\n",
    "    predicted_text = tokenizer.decode(generated_ids)\n",
    "    print(\"Predicted text:\", predicted_text)\n",
    "    return predicted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eedc7a73-74af-450b-8c42-61aabbe9169b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape:  (1, 30)\n",
      "Logits shape:  (1, 30, 49152)\n",
      "Stopping generateion as EOS token reached.\n",
      "Predicted text: <|im_start|>assistant\n",
      "Gravity is a fundamental force of nature that attracts objects with mass towards each other. It is a result of the interaction between mass, energy, and space itself. In the context of our universe, gravity is a result of the curvature of spacetime caused by the presence of mass and energy.\n",
      "\n",
      "Imagine spacetime as a trampoline. When you place a heavy object, like a bowling ball, on the trampoline, it creates a depression in the surface. This depression is caused by the object's mass and the energy it contains. The more massive the object, the larger the depression.\n",
      "\n",
      "Now, when you move an object, such as a bowling ball, it creates a gravitational pull on the surrounding space. This gravitational pull is what causes the bowling ball to move towards the center of the trampoline. The more massive the object, the stronger the gravitational pull.\n",
      "\n",
      "Gravity is a universal force that acts between all objects in the universe, regardless of their mass or energy. It is a fundamental aspect of the universe and plays a crucial role in shaping the behavior of celestial bodies, including planets, stars, and galaxies.\n"
     ]
    }
   ],
   "source": [
    "# Run exported models\n",
    "init_model_path = \"smol_lm_init.onnx\"\n",
    "step_model_path = \"smol_lm_step.onnx\"\n",
    "init_model = ort.InferenceSession(init_model_path)\n",
    "step_model = ort.InferenceSession(step_model_path)\n",
    "\n",
    "inputs = preprocess(\"What is gravity?\", tokenizer)\n",
    "predicted_text = run_models(init_model, step_model, inputs, tokenizer, max_iters=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b09f4e0-c097-4ec8-bde8-a1c38415746d",
   "metadata": {},
   "source": [
    "### Step-4 Quantizing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9a64138-c55c-4fe7-95e2-2d88cbf178d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Dynamic quantization as SmolLM is Transformer based model.\n",
    "\n",
    "def apply_dynamic_quantization(fp32_path):\n",
    "    root, ext = os.path.splitext(fp32_path)\n",
    "    fp32_path_preproc = f\"{root}_preproc{ext}\"        \n",
    "    int8_path_dynamic_quant = f\"{root}_int8_dynamic_quant{ext}\"\n",
    "    \n",
    "    # Firstly, apply shape inference and onnxruntime model optimization before quantizing the model.\n",
    "    quant_pre_process(fp32_path, fp32_path_preproc, skip_symbolic_shape=True)\n",
    "\n",
    "    # Apply dynamic quantization\n",
    "    quantize_dynamic(\n",
    "        model_input=fp32_path_preproc,          # Input ONNX model\n",
    "        model_output=int8_path_dynamic_quant,   # Output quantized model\n",
    "        weight_type=QuantType.QUInt8            # Quantize only weights to uint8, \n",
    "                                                # activations will be quantize during runtime\n",
    "    )\n",
    "    \n",
    "    print(f\"Dynamic Quantized model saved at: {int8_path_dynamic_quant}\")\n",
    "    \n",
    "    # Compare model sizes\n",
    "    fp32_size = os.path.getsize(fp32_path) / 1024 / 1024\n",
    "    dynamic_quant_size = os.path.getsize(int8_path_dynamic_quant) / 1024 / 1024\n",
    "    \n",
    "    print(f\"FP32 Model Size: {fp32_size:.2f} MB\")\n",
    "    print(f\"Dynamic Quantized Model Size: {dynamic_quant_size:.2f} MB\")\n",
    "\n",
    "    return int8_path_dynamic_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c845b76e-9d20-4491-a41e-52ecbb00093a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic Quantized model saved at: smol_lm_init_int8_dynamic_quant.onnx\n",
      "FP32 Model Size: 622.25 MB\n",
      "Dynamic Quantized Model Size: 156.19 MB\n",
      "Dynamic Quantized model saved at: smol_lm_step_int8_dynamic_quant.onnx\n",
      "FP32 Model Size: 622.26 MB\n",
      "Dynamic Quantized Model Size: 156.20 MB\n"
     ]
    }
   ],
   "source": [
    "# Quantize SmolLM init model\n",
    "init_model_path_int8 = apply_dynamic_quantization(init_model_path)\n",
    "\n",
    "# Quantize SmolLM step model\n",
    "step_model_path_int8 = apply_dynamic_quantization(step_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6b2c0bc-f81e-4a77-9c17-2921ac46bb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape:  (1, 30)\n",
      "Logits shape:  (1, 30, 49152)\n",
      "Stopping generateion as EOS token reached.\n",
      "Predicted text: <|im_start|>assistant\n",
      "Gravity is a fundamental force of nature that attracts objects with mass towards each other. It is a result of the interaction between mass, energy, and space. The force of gravity is a result of the curvature of spacetime caused by the presence of mass and energy.\n",
      "\n",
      "The force of gravity is not a force that acts between two objects, but rather a force that acts between objects that are moving towards each other. This is because the presence of mass causes the gravitational field to become stronger, pulling objects towards each other.\n",
      "\n",
      "The strength of the gravitational field depends on the mass of the objects and the distance between them. The stronger the gravitational field, the stronger the force of gravity. The strength of the gravitational field is also affected by the strength of the gravitational field of the other objects, as well as the distance between them.\n",
      "\n",
      "Gravity is a universal force that acts across the entire universe, and it is the reason why objects fall towards the ground, and why planets orbit around stars. It is a fundamental force of nature that is responsible for many natural phenomena, such as the tides, the orbits of planets, and the rotation of the Earth.\n",
      "\n",
      "In summary, gravity is a fundamental force of nature that attracts objects with mass towards each other, and it is a result of the curvature of spacetime caused by the presence of mass and energy.\n"
     ]
    }
   ],
   "source": [
    "# Run quantized models\n",
    "init_model = ort.InferenceSession(init_model_path_int8)\n",
    "step_model = ort.InferenceSession(step_model_path_int8)\n",
    "\n",
    "inputs = preprocess(\"What is gravity?\", tokenizer)\n",
    "predicted_text = run_models(init_model, step_model, inputs, tokenizer, max_iters=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
